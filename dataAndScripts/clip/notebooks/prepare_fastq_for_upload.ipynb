{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, re, glob, pandas, importlib\n",
    "import numpy as np\n",
    "sys.path.append('/Users/dp/pma/')\n",
    "top = top_dir = '/Users/dp/pma/dataAndScripts/clip/meta/'\n",
    "pma_dir = '/Users/dp/pma/'\n",
    "os.chdir(top)\n",
    "import sameRiver\n",
    "import sameRiver.exp\n",
    "importlib.reload(sameRiver.exp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert sizes for GEO upload. Not sure this is needed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = {}\n",
    "with open('tables/insert_sizes.txt') as f:\n",
    "    for li in f:\n",
    "        #print(li)\n",
    "        s = li.rstrip('\\n').split('\\t')\n",
    "        s[1] = '{' + s[1].split(' {')[1].rstrip(')')#re.sub(r\"defaultdict(<class 'int'>, \", \"\", s[1])\n",
    "        s[1] = dict(eval(s[1]))\n",
    "        sizes[s[0]] = s[1]\n",
    "#\n",
    "\n",
    "rows = []\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for k, _dict in sizes.items():\n",
    "    # _dict = {length: number_of_instances}\n",
    "    lengths = np.fromiter(_dict.keys(), dtype=float)\n",
    "    num_obs = np.fromiter(_dict.values(), dtype=float)\n",
    "    #plt.plot(lengths, num_obs, 'k.')\n",
    "    #plt.title(k)\n",
    "    #plt.show()\n",
    "    #plt.clf()\n",
    "    #print(lengths)\n",
    "    total_obs = np.sum(num_obs)\n",
    "    mu = np.sum([n_obs*size for size, n_obs in zip(lengths, num_obs)])/total_obs\n",
    "    var = np.average((lengths - mu)**2, weights=num_obs)\n",
    "    #print(k, mu, np.sqrt(var))\n",
    "    k = os.path.basename(k)\n",
    "    spk = k.split('_')\n",
    "    rows.append({\n",
    "        'file name 1': k + '.gz',\n",
    "        'file name 2': '_'.join([spk[0], spk[1], spk[2], 'R2', spk[3]]) + '.gz',\n",
    "        'average insert size': np.around(mu, decimals=3),\n",
    "        'standard deviation': np.around(np.sqrt(var), decimals=3),\n",
    "    })\n",
    "df = pandas.DataFrame(rows)\n",
    "\n",
    "df = df.loc[:,['file name 1', 'file name 2', 'average insert size', 'standard deviation']]\n",
    "\n",
    "df.to_excel('tables/insert_sizes_for_geo_upload.xlsx')\n",
    "\n",
    "if os.path.exists('tables/insert_sizes_for_geo_upload.xlsx'):\n",
    "    inserts = pandas.read_excel('tables/insert_sizes_for_geo_upload.xlsx')\n",
    "    inserts = inserts.loc[[x in geo['raw file'].tolist() for x in inserts['file name 1']], :]\n",
    "    inserts.to_excel('tables/insert_sizes_for_geo_upload_subset_to_those_included_in_upload.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write an excel file of what datasets were used for what.\n",
    "I think this is not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import sameRiver.scheme\n",
    "\n",
    "\n",
    "from logs.datasets_included_in_positiveCounts_object import datasets_in_positiveCounts\n",
    "from logs.datasets_used_for_biotypes import datasets_for_rbps, datasets_for_randos, datasets_for_pcbp1\n",
    "from logs.datasets_included_in_negativeCounts_object import datasets_in_negativeCounts\n",
    "from logs.datasets_used_for_heatmap import datasets_used_for_heatmap\n",
    "datasets_in_biotypes = set(datasets_for_rbps + datasets_for_randos + datasets_for_pcbp1)\n",
    "def where_am_i_used(name):\n",
    "    used = {\n",
    "        'positiveCounts': '', 'negativeCounts': '', 'biotypes': '', 'heatmap': ''}\n",
    "    if name in datasets_in_negativeCounts:\n",
    "        used['negativeCounts'] = name\n",
    "    if name in datasets_in_positiveCounts:\n",
    "        used['positiveCounts'] = name\n",
    "    if name in datasets_in_biotypes:\n",
    "        used['biotypes'] = name\n",
    "    if name in datasets_used_for_heatmap:\n",
    "        used['heatmap'] = name\n",
    "    used['places_used'] = sum([(x!='') for x in used.values()])\n",
    "    return used\n",
    "\n",
    "scheme = sameRiver.scheme.scheme('./scheme.xlsx')\n",
    "scheme.scheme_df['long_basename'] = [x.split('.fastq')[0] for x in scheme.scheme_df['long_fname_R1']]\n",
    "\n",
    "rows = []\n",
    "for row in scheme.scheme_df.to_dict('records'):\n",
    "    row.update(where_am_i_used(row['long_basename']))\n",
    "    rows.append(row)\n",
    "used_datasets = pandas.DataFrame(rows)\n",
    "used_datasets.to_excel('tables/where_datasets_were_used.xlsx')\n",
    "#print(used_datasets[used_datasets['places_used']==0])\n",
    "#print(scheme.scheme_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Renaming some Rbfox1 files.\n",
    "Not necessary anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, os, pandas\n",
    "def rename_split_r1r2_dir_based_on_scheme(exp_name, prefix, scheme_fname, dir_name, out_dir):\n",
    "    scheme = pandas.read_excel(scheme_fname)\n",
    "    scheme['fname_R1'] = [\n",
    "        '{d}/{p6}/{p6}_{p3}.fastq'.format(d=dir_name, p6=p6_bc, p3=p3_bc) \\\n",
    "        for (p6_bc, p3_bc) in zip(scheme.P6_BC, scheme.P3_BC)]  \n",
    "    scheme['fname_R2'] = [\n",
    "        '{d}/{p6}/{p6}_R2_{p3}.fastq'.format(d=dir_name, p6=p6_bc, p3=p3_bc) \\\n",
    "        for (p6_bc, p3_bc) in zip(scheme.P6_BC, scheme.P3_BC)]\n",
    "\n",
    "    scheme['upload_fname_R1'] = [\n",
    "    '{o}/{exp}_{gene}_{prefix}{p6}_{p3}.fastq'.format(\n",
    "        o=out_dir, prefix=prefix,\n",
    "        exp=exp_name, gene=re.sub(':', '-', gene), p6=p6, p3=p3) for (gene, p6, p3) in zip(\n",
    "            scheme['Gene'], scheme['P6_BC'], scheme['P3_BC'])\n",
    "    ]\n",
    "    scheme['upload_fname_R2'] = [\n",
    "    '{o}/{exp}_{gene}_{prefix}{p6}_R2_{p3}.fastq'.format(\n",
    "        o=out_dir, prefix=prefix,\n",
    "        exp=exp_name, gene=re.sub(':', '-', gene), p6=p6, p3=p3) for (gene, p6, p3) in zip(\n",
    "            scheme['Gene'], scheme['P6_BC'], scheme['P3_BC'])\n",
    "    ]\n",
    "    print(scheme.iloc[0])\n",
    "    print(scheme['upload_fname_R1'].tolist())\n",
    "    \n",
    "    def try_moving(infile, outfile):\n",
    "        if not os.path.exists(os.path.dirname(outfile)):\n",
    "            os.system('mkdir ' + os.path.dirname(outfile))\n",
    "        if not os.path.exists(infile):\n",
    "            print(\"Not found: {}\".format(infile))\n",
    "        else:\n",
    "            if os.path.exists(outfile):\n",
    "                print(\"Exists (skipping): {}\".format(outfile))\n",
    "                return\n",
    "            cmd = 'rsync {} {}'.format(infile, outfile)\n",
    "            print(cmd)\n",
    "            os.system(cmd)\n",
    "\n",
    "    for row in scheme.to_dict('records'):\n",
    "        try_moving(row['fname_R1'], row['upload_fname_R1'])\n",
    "        try_moving(row['fname_R2'], row['upload_fname_R2'])\n",
    "        \n",
    "        \n",
    "rename_split_r1r2_dir_based_on_scheme(\n",
    "    'Exp73', '73', '/Users/dfporter/clip/miseq/Runs/190413rbfox/scheme.xlsx',\n",
    "    '/Users/dfporter/clip/miseq/Runs/190413rbfox/fastq/r1r2_split/',\n",
    "    '/Users/dfporter/clip/miseq/Runs/190413rbfox/fastq/r1r2_split_renamed/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcbp1s = [\n",
    "    'PCBP1',\n",
    "    'PCBP1 100P', 'PCBP1 100Q', 'PCBP1 dKH',\n",
    "    'PCBP1:100P', 'PCBP1:100Q','PCBP1:dKH',\n",
    "    'PCBP1-100P', 'PCBP1-100Q','PCBP1-dKH',\n",
    "]\n",
    "randos = [\n",
    "    'UBA2', 'ETS2', 'CAPNS6', 'TPGS2', \n",
    "    'EPB41L5', 'CHMP3', 'CDK4', 'IDE', \n",
    "    'DCTN6', 'CCIN', 'ITPA', \n",
    "]\n",
    "rbps_endog = ['FBL', 'hnRNPC', 'Rbfox2']\n",
    "uORF = ['CELF1', 'Rbfox1',  'hnRNPD']\n",
    "pSLX3 = [\n",
    "    'NUFIP', 'NUFP1', 'NUFIP-R474W', 'NUFIP1-R474W',\n",
    "    'DICER1', 'DICER1-R944Q',\n",
    "    'CNOT9', 'CNOT9-P131L',\n",
    "]\n",
    "pSLX4 = [\n",
    "    'RBFOX1', 'RBFOX1-A69T',\n",
    "    'RARS2', 'RARS2-R6C',\n",
    "    'RPL5', 'RPL5-E82K',\n",
    "]\n",
    "pSLX3 = [\n",
    "    'SMAD4', 'SMAD4-R361H',\n",
    "    'SMAD3',\n",
    "    'CRNKL1', 'CRNKL1-S128F',\n",
    "    'BRCA1', 'BARD1',\n",
    "]\n",
    "NpL = [\n",
    "    'RBM11', 'RBM11-R74C',\n",
    "    'A1CF', 'A1CF-E34K',\n",
    "    'NOVA1', 'NOVA1-R146C',\n",
    "    'RBM39', 'RBM39-T353I',\n",
    "    'FUBP1', 'FUBP1-R429C',\n",
    "    'CNOT1', 'CNOT1-R499H',\n",
    "    'KHDRBS2', 'KHDRBS2-R168C',\n",
    "    # ...\n",
    "    'DDX3X', 'DDX3X-R528C',\n",
    "]\n",
    "#    'KPNB1', 'KPNB1-L552F'\n",
    "#rbps_pLEX = ['SF3B1']\n",
    "\n",
    "def to_expression_type(_type):\n",
    "    #print(_type)\n",
    "    if _type in pcbp1s:\n",
    "        return 'FLAG-HA-HIS tagged ORF CRISPR targetted to AAVS1 safe harbor locus with Puro selection'\n",
    "    if _type in randos:\n",
    "        return 'FLAG-HA-HIS tagged ORF transiently expressed from pLEX vector 2 days without selection'\n",
    "    if _type in NpL:\n",
    "        return 'FLAG-HA-HIS tagged ORF transiently expressed from pLEX vector 1-2 days without selection'\n",
    "    if _type in rbps_endog:\n",
    "        return 'Immunopurified endogenous protein (unmodified cells)'\n",
    "    if _type in pSLX3:\n",
    "        return 'N-terminally FLAG-HA-HIS tagged ORF expressed from a pLEX-derived vector (pSLX3).'\n",
    "    if _type in pSLX4:\n",
    "        return 'C-terminally FLAG-HA-HIS tagged ORF expressed from a pLEX-derived vector (pSLX4).'\n",
    "    if _type in uORF:\n",
    "        return 'FLAG-HA-HIS tagged ORF expressed from pLEX with a uORF in front of CDS to lower expression levels'\n",
    "    if _type == 'HCT116':\n",
    "        return 'Unmodified cells, immunopurified with anti-HA but without epitope'\n",
    "    if re.sub('[:-]', ' ', _type) in ['SF3B1', 'SF3B1 K700E']:\n",
    "        return '3X FLAG tagged SF3B1 expressed from pLEX vector for 2 days without selection'\n",
    "    if 'STD' in _type:\n",
    "        return 'cDNA standard for library quantification.'\n",
    "    if _type == 'No-vector':\n",
    "        return 'Unmodified cells, not expressing any epitope.'\n",
    "    #if _type in ['100Pox', '100Qox', 'PCBP1ox']:\n",
    "    print(f\"Couldn't find {_type}\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File 2 Description of datasets currently has the headers:\n",
    "```text\n",
    "    title\tsource name\tExpression\tL3-BC\tL5-BC\traw file\traw file read mate\tRead 1 file checksum\tinstrument model\tread length\tsingle or paired-end\n",
    "\n",
    "    easyCLIP CCIN Replicate 1\tHEK293T cells\tFLAG-HA-HIS tagged ORF transiently expressed from pLEX vector 2 days without selection\tTCA\tCTGATC\tExp56_CCIN_tcCTGATC_TCA.fastq.gz\tExp56_CCIN_tcCTGATC_R2_TCA.fastq.gz\t1d3ad6fbda45862d06caa07f30c930a4\tIllumina Miseq\t76\tpaired-end\n",
    "```\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sameRiver.readsPerGene import *\n",
    "\n",
    "os.chdir(top)\n",
    "\n",
    "geo_upload_columns = [\n",
    "    'Sample name', 'title', 'source name', 'organism',\n",
    "    'characteristics: Expression',\n",
    "    'characteristics: Exp',\n",
    "    'characteristics: L3-BC', \n",
    "    'characteristics: L5-BC',\n",
    "    'molecule', 'description', 'processed data file', 'raw file']\n",
    "\n",
    "scheme_fname = f'{top}/scheme.xlsx'\n",
    "geo_fname = f'{pma_dir}/doc/seq_template_v2.1.xls'\n",
    "\n",
    "# Just to use the blacklist and column names.\n",
    "rpg = readsPerGene(f'{top}/ann_counts.txt')\n",
    "black_list = rpg.set_blacklist()\n",
    "\n",
    "# Read the scheme to serve as a template for the run information table.\n",
    "scheme = pandas.read_excel(scheme_fname)\n",
    "print('>>>', len(scheme.index))\n",
    "scheme.drop_duplicates(subset='long_fname_R1', inplace=True)\n",
    "print(len(scheme.index))\n",
    "scheme['Gene'] = [re.sub(':', '-', x) for x in scheme.Gene]\n",
    "\n",
    "# Make the run information dataframe and give it a numeric index.\n",
    "geo = scheme.copy()\n",
    "geo['Sample name'] = range(1, len(geo.index)+1)\n",
    "\n",
    "# Add information to the table.\n",
    "geo['source name'] = [\n",
    "    {True: 'HCT116 cells', False: 'HEK293T cells'}[\n",
    "        bool('PCBP1' in protein or 'HCT116' in protein)] for protein in geo['Gene']]\n",
    "geo['organism'] = 'H. sapiens'\n",
    "geo['characteristics: Expression'] = [to_expression_type(x) for x in geo['Gene']]\n",
    "geo['characteristics: Exp'] = geo['Experiment']\n",
    "geo['characteristics: Protein'] = geo['Gene']\n",
    "geo['characteristics: L3-BC'] = geo['P3_BC']\n",
    "geo['characteristics: L5-BC'] = [x for x in geo['P6_BC']]\n",
    "geo['molecule'] = 'Library from RNA UV cross-linked to the indicated protein'\n",
    "geo['processed data file'] = 'ann_counts.txt'\n",
    "geo['basename'] = ['_'.join([str(_) for _ in x]) for x in zip(\n",
    "    geo['Experiment'], geo['Gene'], geo['P6_BC'], geo['P3_BC'])]\n",
    "geo['basename_r2'] = ['{}_{}_{}_R2_{}'.format(*x) for x in zip(\n",
    "    geo['Experiment'], geo['Gene'], geo['P6_BC'], geo['P3_BC'])]\n",
    "\n",
    "def purged(name):\n",
    "    \"\"\"\n",
    "    black_list = [\n",
    "        'Exp16_FBL_24AGCTAG_CAG',  # Very small dataset.\n",
    "        'Exp16_FBL_24AGCTAG_AAC',  # I believe this was also discarded because it was too small.\n",
    "        'Exp16_hnRNPC_24TGAGTG_AGT', # Unknown why this was blacklisted.\n",
    "        'Exp16_hnRNPC_24TGAGTG_CAG', # Unknown why this was blacklisted.\n",
    "        'Exp28_hnRNPC_17CGATTA_AAC', # Unknown why this was blacklisted.\n",
    "        'Exp31_UBA2_15TGAGTG_AAC',# Too correlated with CDK4 AAC Exp31.\n",
    "        'Exp31_UBA2_15TGAGTG_CAG', # Empty.\n",
    "        'Exp31_UBA2_05TGAGTG_CAG',\n",
    "        'Exp31_CDK4_15GCCATG_AAC', # Too correlated with UBA2 AAC Exp31.\n",
    "        'Exp31_CDK4_15GCCATG_CAG', # Empty.\n",
    "        'Exp33_CDK4_17GCCATG_AGT',\n",
    "        'Exp31_CDK4_05GCCATG_CAG',\n",
    "        'Exp33_CAPNS6_17CACTGT_TCA', # Empty.\n",
    "        'Exp61_PCBP1-100P_hpGCCATG_TCA',  # Too small.\n",
    "        'Exp61_PCBP1-100P_hpGCCATG_AGT',  # Too small.\n",
    "        ]\"\"\"\n",
    "    \n",
    "    #unused_datasets = used_datasets.loc[[x<1 for x in used_datasets['places_used']], :]['long_basename'].tolist()\n",
    "    #unused_datasets = [x for x in unused_datasets if not re.search('No[_ ]vector', x, re.IGNORECASE)]\n",
    "\n",
    "    #black_list.extend(unused_datasets)\n",
    "    if 'BRCA' in name:\n",
    "        print(f\"checking {name}\")\n",
    "    if name not in rpg.df.columns:\n",
    "        print(\"Not in rpg.df.columns\")\n",
    "        return False\n",
    "    if name in rpg.black_list:\n",
    "        return False\n",
    "    if 'STD' in name:\n",
    "        return False\n",
    "    if 'BRCA' in name:\n",
    "        print(f\"Found {name}\")\n",
    "    return True\n",
    "\n",
    "print(f\"{[x for x in geo['basename'] if 'BRCA' in x]}\")\n",
    "print(f\"{[x for x in rpg.df.columns if 'BRCA' in x]}\")\n",
    "print(f\"Before purge: {len(geo.index)} datasets.\")\n",
    "geo = geo.loc[[purged(name) for name in geo['basename']], :]\n",
    "print(f\"After purge: {len(geo.index)} datasets.\")\n",
    "print(f\"{[x for x in geo['basename'] if 'BRCA' in x]}\")\n",
    "\n",
    "rep_n = {}\n",
    "def add_replicate(name):\n",
    "    if name in rep_n:\n",
    "        rep_n[name] += 1\n",
    "    else:\n",
    "        rep_n[name] = 1\n",
    "    return 'easyCLIP {} Replicate {}'.format(name, rep_n[name])\n",
    "\n",
    "geo['title'] = [add_replicate(protein) for protein in geo['Gene']]\n",
    "geo['raw file'] = ['{}.fastq.gz'.format(x) for x in geo.basename]\n",
    "geo['raw file read mate'] = ['{}.fastq.gz'.format(x) for x in geo.basename_r2]\n",
    "\n",
    "geo_upload_columns += ['raw file read mate']\n",
    "geo = geo.loc[:, [(x in geo_upload_columns) for x in geo.columns]]\n",
    "geo = geo.loc[:, geo_upload_columns]\n",
    "\n",
    "geo.index = geo['Sample name']\n",
    "geo.to_excel(f'{top}/tables/seq_template_generated_sample_list.xlsx')\n",
    "geo.to_excel(f'{top}/tables/File 2 Description of datasets.xlsx')\n",
    "\n",
    "df = pandas.read_excel(f'{top}/tables/seq_template_generated_sample_list.xlsx')\n",
    "\n",
    "fastq_unzip_dir = 'geo_upload_prezip/'\n",
    "fastq_zip_dir = 'geo_upload/'\n",
    "server_directory = './kkunqu/'\n",
    "password = '33%9uyj_fCh?M16H'\n",
    "upload_commands = open('upload_commands.sh', 'w')\n",
    "zip_commands = open('zip_commands.sh', 'w')\n",
    "for fname1_unzip, fname2_unzip in zip(df['raw file'], df['raw file read mate']):\n",
    "    \n",
    "    fname1_unzip = fastq_unzip_dir + fname1_unzip.rstrip('.gz')\n",
    "    fname2_unzip = fastq_unzip_dir + fname2_unzip.rstrip('.gz')\n",
    "    fname1 = fastq_zip_dir + '/' + os.path.basename(fname1_unzip) + '.gz'\n",
    "    fname2 = fastq_zip_dir + '/' + os.path.basename(fname2_unzip) + '.gz'    \n",
    "\n",
    "    zip_commands.write(f'gzip < {fname1_unzip} > {fname1} \\n')\n",
    "    zip_commands.write(f'gzip < {fname2_unzip} > {fname2} \\n')\n",
    "    for local_file_or_dir in [fname1, fname2]:\n",
    "        if not os.path.exists(local_file_or_dir):\n",
    "            #print(\"Could not find file {} to upload.\".format(local_file_or_dir))\n",
    "            pass\n",
    "        \n",
    "        cmd = \"ncftpput -F -R -z -u geo \"\n",
    "        cmd += '-p \"{}\" ftp-private.ncbi.nlm.nih.gov '.format(password)\n",
    "        cmd += ' {} {} '.format(server_directory, local_file_or_dir)\n",
    "        upload_commands.write(cmd +'\\n')\n",
    "        \n",
    "upload_commands.close()\n",
    "zip_commands.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "geo = pandas.read_excel(geo_fname, sheet_name='Samples')\n",
    "\n",
    "geo['geo_id'] = [','.join([str(x).split('.')[0] for x in arr]) for arr in zip(\n",
    "    geo['characteristics: L3-BC'], geo['characteristics: L5-BC'], geo['characteristics: Run_P6BC'])]\n",
    "\n",
    "desired_geo_ids = set(geo.geo_id)\n",
    "scheme['geo_id'] = [','.join([str(x).split('.')[0] for x in arr]) for arr in zip(\n",
    "    scheme['L3'], scheme['L5'], scheme['P6_BC'])]\n",
    "geo['Basename'] = [ '_'.join(arr) for arr in zip(\n",
    "    geo['characteristics: Exp'], geo['characteristics: L3-BC'], geo['characteristics: L5-BC'], geo['characteristics: Run_P6BC']\n",
    ")]\n",
    "scheme = scheme[[(x in desired_geo_ids) for x in scheme.geo_id]]\n",
    "\n",
    "to_dir = {\n",
    "    '05': '/Volumes/Seagate/hiseq/180105/fastq/r1r2_split/',\n",
    "    '17': '/Volumes/Seagate/hiseq/180117/fastq/r1r2_split/',\n",
    "    'tc': '/Volumes/Seagate/Miseq/Runs/181115/fastq/r1r2_split/',\n",
    "    'pc': '/Users/dfporter/pma/dataAndScripts/clip/miseq/Runs/190113/fastq/r1r2_split/',\n",
    "    '24':'/Volumes/Seagate/hiseq/170924/fastq/r1r2_split/',\n",
    "}\n",
    "\n",
    "scheme['dir'] = [to_dir[p6bc[:2]] for p6bc in scheme.P6_BC]\n",
    "\n",
    "scheme['fname_R1'] = [\n",
    "    '{d}/{p6}/{p6}_{p3}.fastq'.format(d=_dir, p6=p6_bc[2:], p3=p3_bc) \\\n",
    "    for (_dir, p6_bc, p3_bc) in zip(scheme.dir, scheme.P6_BC, scheme.P3_BC)]\n",
    "\n",
    "scheme['fname_R2'] = [\n",
    "    '{d}/{p6}/{p6}_R2_{p3}.fastq'.format(d=_dir, p6=p6_bc[2:], p3=p3_bc) \\\n",
    "    for (_dir, p6_bc, p3_bc) in zip(scheme.dir, scheme.P6_BC, scheme.P3_BC)]\n",
    "\n",
    "scheme['upload_fname_R1'] = [\n",
    "    '{o}/{exp}_{gene}_{p6}_{p3}.fastq'.format(\n",
    "        o='/Volumes/Seagate/hiseq/to_upload/',\n",
    "        exp=exp, gene=re.sub(':', '-', gene), p6=p6, p3=p3) for (exp, gene, p6, p3) in zip(\n",
    "            scheme['Experiment'], scheme['Gene'], scheme['P6_BC'], scheme['P3_BC'])\n",
    "]\n",
    "scheme['upload_fname_R2'] = [\n",
    "    '{o}/{exp}_{gene}_{p6}_R2_{p3}.fastq'.format(\n",
    "        o='/Volumes/Seagate/hiseq/to_upload/',\n",
    "        exp=exp, gene=re.sub(':', '-', gene), p6=p6, p3=p3) for (exp, gene, p6, p3) in zip(\n",
    "            scheme['Experiment'], scheme['Gene'], scheme['P6_BC'], scheme['P3_BC'])\n",
    "]\n",
    "#print(scheme.upload_fname_R1)\n",
    "\n",
    "def try_moving(infile, outfile):\n",
    "    if not os.path.exists(infile):\n",
    "        print(\"Not found: {}\".format(infile))\n",
    "    else:\n",
    "        if os.path.exists(outfile):\n",
    "            print(\"Exists (skipping): {}\".format(outfile))\n",
    "            return\n",
    "        cmd = 'rsync {} {}'.format(infile, outfile)\n",
    "        print(cmd)\n",
    "        os.system(cmd)\n",
    "\n",
    "#for row in scheme.to_dict('records'):\n",
    "#    try_moving(row['fname_R1'], row['upload_fname_R1'])\n",
    "#    try_moving(row['fname_R2'], row['upload_fname_R2'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAW FILES table\n",
    "# GEO columns: \n",
    "\n",
    "md5 = pandas.read_csv(f'{top}/tables/md5values.txt', sep=' ')\n",
    "md5['file_checksum'] = md5.index\n",
    "md5['raw_file'] = [os.path.basename(x) for x in md5.raw_file]\n",
    "to_md5 = dict(zip(md5['raw_file'], md5['file_checksum']))\n",
    "\n",
    "\n",
    "run_code_to_instrument_and_read_len = {\n",
    "    'mg': {'Instrument': 'Illumina Hiseq 2500+Illumina Miseq', 'read length': 0},\n",
    "    'rb': {'Instrument': 'Illumina Hiseq 4000', 'read length': 76},\n",
    "    'hp': {'Instrument': 'Illumina Hiseq 4000', 'read length': 76},\n",
    "    '05': {'Instrument': 'Illumina Hiseq 2500', 'read length': 125},\n",
    "    '17': {'Instrument': 'Illumina Hiseq 2500', 'read length': 125},\n",
    "    '24': {'Instrument': 'Illumina Hiseq 2500', 'read length': 125},\n",
    "    'pc': {'Instrument': 'Illumina Miseq', 'read length': 76},\n",
    "    'tc': {'Instrument': 'Illumina Miseq', 'read length': 76},\n",
    "}\n",
    "\n",
    "def fname_to_sequencing_run(fname):\n",
    "    s = os.path.basename(fname).split('_')\n",
    "    run_code = s[2][:2]\n",
    "    return run_code_to_instrument_and_read_len[run_code]['Instrument']\n",
    "\n",
    "def fname_to_read_length(fname):\n",
    "    s = os.path.basename(fname).split('_')\n",
    "    run_code = s[2][:2]\n",
    "    return run_code_to_instrument_and_read_len[run_code]['read length']\n",
    "\n",
    "df = pandas.read_excel('tables/seq_template_generated_sample_list.xlsx', index_col=0)\n",
    "\n",
    "rows = []\n",
    "for row in df.to_dict('records'):\n",
    "    rows.append({\n",
    "        'file name': row['raw file'],\n",
    "        'file type': 'fastq',\n",
    "        'instrument model': fname_to_sequencing_run(row['raw file']),\n",
    "        'read length': fname_to_read_length(row['raw file']),\n",
    "        'single or paired-end': 'paired-end',\n",
    "        'file checksum': to_md5.get(row['raw file'], '')\n",
    "    })\n",
    "    rows.append({\n",
    "        'file name': row['raw file read mate'],\n",
    "        'file type': 'fastq',\n",
    "        'instrument model': fname_to_sequencing_run(row['raw file read mate']),\n",
    "        'read length': fname_to_read_length(row['raw file']),\n",
    "        'single or paired-end': 'paired-end',\n",
    "        'file checksum': to_md5.get(row['raw file read mate'], '')\n",
    "    })\n",
    "df = pandas.DataFrame(rows)\n",
    "df = df.loc[:, [\"file name\", \"file type\", \"file checksum\", \"instrument model\", \"read length\", \"single or paired-end\"]]\n",
    "\n",
    "df.to_excel('tables/raw_files_table_for_geo_upload.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
